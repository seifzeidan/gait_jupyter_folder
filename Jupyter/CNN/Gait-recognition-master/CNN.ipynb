{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/Users/luffy/Desktop/gait_test/train/train_acc_x.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m input_types\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_x.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_y.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_z.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyr_x.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyr_y.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyr_z.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     55\u001b[0m train_paths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/luffy/Desktop/gait_test/train/train_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m input_types]\n\u001b[1;32m---> 56\u001b[0m train_np\u001b[38;5;241m=\u001b[39m\u001b[43mXload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m test_paths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/luffy/Desktop/gait_test/test/test_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m input_types]\n\u001b[0;32m     58\u001b[0m test_np\u001b[38;5;241m=\u001b[39mXload(test_paths)\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mXload\u001b[1;34m(paths)\u001b[0m\n\u001b[0;32m     28\u001b[0m data_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m---> 30\u001b[0m     temp\u001b[38;5;241m=\u001b[39mdata_normalize(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m######合并acc_x/y/z,gyr_x/y/z\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1336\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1338\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    973\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 975\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: /Users/luffy/Desktop/gait_test/train/train_acc_x.txt not found."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Fri Apr 27 20:51:42 2018\n",
    "\n",
    "@author: luffy\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "####################Load data############################\n",
    "\n",
    "#Data preprocessing\n",
    "\n",
    "def data_normalize(data_temp):\n",
    "    data_temp2=data_temp.T\n",
    "    data_temp2 -=np.mean(data_temp2,axis=0)\n",
    "    data_temp2 /=np.std(data_temp2,axis=0)\n",
    "    data_temp=data_temp2.T\n",
    "    return data_temp\n",
    "\n",
    "def Xload(paths):\n",
    "    data_list=[]\n",
    "    for path in paths:\n",
    "        temp=data_normalize(np.loadtxt(path))\n",
    "        data_list.append(temp)\n",
    "    ######合并acc_x/y/z,gyr_x/y/z\n",
    "    lenth=len(data_list[0])\n",
    "    width=len(data_list[0][1])\n",
    "    data_np=np.empty((lenth,width*6))\n",
    "    for i in range(width*6):\n",
    "        temp= i%6\n",
    "        data_np[:,i]=data_list[temp][:,i//6]\n",
    "    return data_np\n",
    "\n",
    "def Yload(path):\n",
    "    train_np= np.loadtxt(path)  \n",
    "    train_np1=list(train_np)\n",
    "    train_np2=list(map(int, train_np1))\n",
    "    ytrain_np=[train_value-1 for train_value in train_np2]\n",
    "    #Convert tags to onehot\n",
    "    batch_size = tf.size(ytrain_np)\n",
    "    labels = tf.expand_dims(ytrain_np, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat([indices, labels],1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 21]), 1.0, 0.0)\n",
    "    return onehot_labels.eval()\n",
    "    \n",
    "input_types= [\"acc_x.txt\",\"acc_y.txt\",\"acc_z.txt\",\"gyr_x.txt\",\"gyr_y.txt\",\"gyr_z.txt\"]\n",
    "train_paths=[\"/Users/luffy/Desktop/gait_test/train/train_\"+ i for i in input_types]\n",
    "train_np=Xload(train_paths)\n",
    "test_paths=[\"/Users/luffy/Desktop/gait_test/test/test_\"+ i for i in input_types]\n",
    "test_np=Xload(test_paths)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "ytrain_path= \"/Users/luffy/Desktop/gait_test/train/y_train.txt\"\n",
    "ytrain_label=Yload(ytrain_path)\n",
    "ytest_path= \"/Users/luffy/Desktop/gait_test/test/y_test.txt\"\n",
    "ytest_label=Yload(ytest_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################Set parameters###############################\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) # 变量的初始值为截断正太分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 128*6])  \n",
    "x_image = tf.reshape(x, [-1,16,8,6]) #reshape\n",
    "\n",
    "\"\"\"\n",
    "# first layer\n",
    "\"\"\"\n",
    "W_conv1 = weight_variable([5, 5, 6, 32])  \n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\"\"\"\n",
    "# second layer\n",
    "\"\"\"\n",
    "W_conv2 = weight_variable([5, 5, 32, 64]) \n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# third layer, the fully connected layer with an input dimension of 4*2*64 and an output dimension of 1024\n",
    "\"\"\"\n",
    "W_fc1 = weight_variable([4*2*64, 1024])  \n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*2*64]) \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\"\"\"\n",
    "# fourth layer，with an input dimension of 1024 and an output dimension of 21，corresponding to 21 categories\n",
    "\"\"\"\n",
    "W_fc2 = weight_variable([1024, 21])\n",
    "b_fc2 = bias_variable([21])\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) #Use softmax as a multi-class activation function\n",
    "y_ = tf.placeholder(tf.float32, [None, 21])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1])) # Loss function，cross entropy\n",
    "tf.summary.scalar('loss', cross_entropy)\n",
    "train_optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # Use Adam optimizer\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "log_dir='/Users/luffy/Desktop/gait_test/tensorboard/'  # Use tensorboard\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir+'test/')\n",
    "sess.run(tf.initialize_all_variables())  # Variable initialization\n",
    "optimal_accuracy = 0.0\n",
    "#train\n",
    "for i in range(10000):  \n",
    "    batch_x,batch_y = next_batch(50,train_np,ytrain_label)  \n",
    "    if i%100 == 0:\n",
    "        loss,test_accuracy,test_summary=sess.run([cross_entropy,accuracy,merged],feed_dict={\n",
    "            x:test_np, y_: ytest_label, keep_prob: 1.0})\n",
    "        test_writer.add_summary(test_summary, i)\n",
    "        print(\"step %d, test accuracy: %g, loss: %g\"%(i, test_accuracy,loss))\n",
    "        optimal_accuracy = max(optimal_accuracy, test_accuracy)\n",
    "    summary_, _ = sess.run([merged,train_optimizer],feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})   \n",
    "    train_writer.add_summary(summary_, i)\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print(\"best test accuracy: %g\"%(optimal_accuracy))\n",
    "print(\"final test accuracy %g\"%accuracy.eval(feed_dict={x: test_np, y_: ytest_label, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final test accuracy %g\"%test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='/Users/luffy/Desktop/gait_test/tensorboard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

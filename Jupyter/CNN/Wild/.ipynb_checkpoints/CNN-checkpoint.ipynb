{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_X(path):\n",
    "    X_signals = []\n",
    "    files = os.listdir(path)\n",
    "    files.sort(key=str.lower)\n",
    "    #['train_acc_x.txt', 'train_acc_y.txt', 'train_acc_z.txt', 'train_gyr_x.txt', 'train_gyr_y.txt', 'train_gyr_z.txt']\n",
    "    for my_file in files:\n",
    "        fileName = os.path.join(path,my_file)\n",
    "        file = open(fileName, 'r')\n",
    "        X_signals.append(\n",
    "            [np.array(cell, dtype=np.float32) for cell in [\n",
    "                row.strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "        #X_signals = 6*totalStepNum*128\n",
    "    X_signals = np.transpose(np.array(X_signals), (1, 0, 2))#(totalStepNum*6*128)\n",
    "    return X_signals.reshape(-1,6,128,1)#(totalStepNum*6*128*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    y_ = y_ - 1\n",
    "    #one_hot\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "X_ = tf.placeholder(tf.float32, [None, 6, 128, 1],name='cnn_X')\n",
    "label_ = tf.placeholder(tf.float32, [None, 118],name='cnn_Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nput shape [batch, in_height, in_width, in_channels]<br>\n",
    "ernel shape [filter_height, filter_width, in_channels, out_channels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\t1*9<br>\n",
    "\tstride = 2 <br>\n",
    "\tpadding<br>\n",
    "\t6*128->6*64*32<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([1, 9, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(\n",
    "    tf.nn.conv2d(X_, W_conv1, strides=[1, 1, 2, 1], padding='SAME') + b_conv1)\n",
    "'''\n",
    "\tpooling\n",
    "\t6*64*32->6*32*32\n",
    "'''\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='VALID')\n",
    "'''\n",
    "\t1*3\n",
    "\tstride = 1\n",
    "\t6*32*32->6*32*64\n",
    "'''\n",
    "W_conv2 = weight_variable([1, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(\n",
    "    tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "'''\n",
    "\t1*3\n",
    "\tstride = 1\n",
    "\tpadding\n",
    "\t6*32*64->6*32*128\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv3 = weight_variable([1, 3, 64, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(\n",
    "    tf.nn.conv2d(h_conv2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\tpooling<br>\n",
    "\t6*32*128->6*16*128<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2 = tf.nn.max_pool(h_conv3, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1],padding='VALID')\n",
    "'''\n",
    "\t6*1\n",
    "\t6*32*128->1*16*128\n",
    "'''\n",
    "W_conv4 = weight_variable([6, 1, 128, 128])\n",
    "b_conv4 = bias_variable([128])\n",
    "h_conv4 = tf.nn.relu(\n",
    "    tf.nn.conv2d(h_pool2, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4)\n",
    "'''\n",
    "\tinput flat 16*128=2048\n",
    "\toutput 20\n",
    "'''\n",
    "h_flat = tf.contrib.layers.flatten(h_conv4)\n",
    "cnn_output = tf.multiply(h_conv4,1,name='cnn_output')\n",
    "W_fc = weight_variable([2048, 118])\n",
    "b_fc = bias_variable([118])\n",
    "h_fc = tf.nn.softmax(tf.matmul(h_flat, W_fc) + b_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(label_ * tf.log(h_fc+1e-10), reduction_indices=[1]),name='cnn_loss')\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(h_fc, 1), tf.argmax(label_, 1),name='cnn_pre_Y')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='cnn_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_X('./data(118)/train/record')\n",
    "X_test = load_X('./data(118)/test/record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = load_y('./data(118)/train/label.txt')\n",
    "test_label = load_y('./data(118)/test/label.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./cnn_ckpt'):\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./cnn_ckpt/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "f = open('./result_cnn.txt','w')\n",
    "for i in range(200):\n",
    "    l = len(train_label)\n",
    "    batch_idxs = int(l / batch_size)\n",
    "    index = list(range(l))\n",
    "    random.shuffle(index)\n",
    "    for idx in range(batch_idxs):\n",
    "        image_idx = X_train[index[idx * batch_size:(idx + 1) * batch_size]]\n",
    "        label_idx = train_label[index[idx * batch_size:(idx + 1) * batch_size]]\n",
    "        #print(start,end)\n",
    "        acc, loss, _ = sess.run([accuracy, cross_entropy, train_step], feed_dict={\n",
    "            X_: image_idx,\n",
    "            label_: label_idx\n",
    "        })\n",
    "        if idx % 100 == 0:\n",
    "            print(str(i) + 'the cross_entropy:', str(loss), 'train_accuracy:', str(acc))\n",
    "            f.write(str(i) + 'the cross_entropy:'+str(loss)+'train_accuracy:'+str(acc))\n",
    "        # Test completely at every epoch: calculate accuracy\n",
    "    accuracy_out, loss_out = sess.run(\n",
    "        [accuracy, cross_entropy],\n",
    "        feed_dict={\n",
    "            X_: X_test,\n",
    "            label_: test_label\n",
    "        }\n",
    "    )\n",
    "    if accuracy_out > best_accuracy:\n",
    "        saver.save(sess,'./cnn_ckpt/model')\n",
    "        best_accuracy = accuracy_out\n",
    "    print(str(i)+'--------------the cross_entropy:', str(loss_out), '-----------------------test_accuracy:', str(accuracy_out))\n",
    "    f.write(str(i)+'--------------the cross_entropy:'+str(loss_out)+'-----------------------test_accuracy:'+str(accuracy_out))\n",
    "print(\"best accuracy:\"+str(best_accuracy))\n",
    "f.write(\"best accuracy:\"+str(best_accuracy))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
